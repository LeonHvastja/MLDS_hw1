{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41607861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c192bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tki-resistance.csv\")\n",
    "data[\"Class\"] = data[\"Class\"].map({\"Bcr-abl\":0, \"Wild type\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6119208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tki-resistance.csv\")\n",
    "data[\"Class\"] = data[\"Class\"].map({\"Bcr-abl\":0, \"Wild type\":1})\n",
    "X, y = np.array(data)[:,0:-1], np.array(data)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d7ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_columns(X, rand):\n",
    "    return range(X.shape[1])\n",
    "\n",
    "class Tree:\n",
    "\n",
    "    def __init__(self, rand=None,\n",
    "                 get_candidate_columns=all_columns,\n",
    "                 min_samples=2):\n",
    "        self.rand = rand  # for replicability\n",
    "        self.get_candidate_columns = get_candidate_columns  # needed for random forests\n",
    "        self.min_samples = min_samples \n",
    "    \n",
    "    def build(self, X: np.array, y: np.array):\n",
    "        \"\"\"\n",
    "        Recusrively build a tree, stop recursion when a split has a child node with gini impurity 0 or\n",
    "        when we have less than min_samples samples.\n",
    "        \"\"\"\n",
    "        # should probably add some checks that the target vector entires are 1 and 0.\n",
    "        \n",
    "        # just for robustness and testing\n",
    "        assert (len(X) == len(y)), \"The input data and label vector are not of equal length\" \n",
    "        \n",
    "        if (len(y) < self.min_samples): # we are in a leaf node\n",
    "            return TreeNode(None, None, round(np.mean(y))) # make the majority class the prediction for this node\n",
    "        if (np.all(y == 1)): # check if we have a node with all ones\n",
    "            return TreeNode(None, None, 1)\n",
    "        if (np.all(y == 0)):\n",
    "            return TreeNode(None, None, 0)\n",
    "        \n",
    "        decision_rule = self.find_decision_rule(X, y)\n",
    "        feature, split_value = decision_rule\n",
    "        \n",
    "        left_i = np.where(X[:,feature] < split_value)\n",
    "        right_i = np.where(X[:, feature] >= split_value)\n",
    "        \n",
    "        left_subtree = Tree()\n",
    "        right_subtree = Tree()\n",
    "        \n",
    "        return TreeNode(left_subtree.build(X[left_i], y[left_i]),right_subtree.build(X[right_i], y[right_i]), decision_rule) \n",
    "    \n",
    "    \n",
    "    def find_decision_rule(self, X, y):\n",
    "        \"\"\"\n",
    "        Input: X - data, y - labels\n",
    "        Output: A tuple (left, right, decision_rule), left indicies, right indicies and rule. Rule itself is a tuple\n",
    "        of the index of the feature to split on and the value of where to split.)\n",
    "        \"\"\"\n",
    "        decision_rule = None\n",
    "        best_info_gain = 0\n",
    "\n",
    "        for feature in self.get_candidate_columns(X, self.rand):\n",
    "            values = X[:, feature]\n",
    "            sorted_indices = np.argsort(values)\n",
    "            sorted_values = values[sorted_indices]\n",
    "            for i in range(len(sorted_values) - 1):\n",
    "\n",
    "                current_info_gain = self.information_gain(y[sorted_indices], np.arange(0,i+1), np.arange(i+1, len(values)))\n",
    "                \n",
    "                if(current_info_gain > best_info_gain):\n",
    "                    best_info_gain = current_info_gain\n",
    "                    decision_rule = (feature, split_value)\n",
    "\n",
    "        return decision_rule\n",
    "                \n",
    "    def midpoint(self, index, y):\n",
    "        \"\"\"Finds the average value of entires at index i and i+1 in a presumably sorted array.\"\"\"\n",
    "        return (y[index] + y[index + 1])/2\n",
    "    \n",
    "    def information_gain(self, y , left_partition_indicies, right_partition_indicies):\n",
    "        \"\"\"\n",
    "        Input: Takes an array of labels and the indicies of which belong to the lefr and right partition.\n",
    "        Output: Returns information gain for this particular split.\n",
    "        \"\"\"\n",
    "        n_left = len(left_partition_indicies)\n",
    "        n_right = len(right_partition_indicies)\n",
    "        n = n_left + n_right\n",
    "\n",
    "        l_weight = n_left/n\n",
    "        r_weight = n_right/n\n",
    "\n",
    "        inf_gain = (self.gini_impurity(y) \n",
    "                    - self.gini_impurity(y[left_partition_indicies])*l_weight \n",
    "                    - self.gini_impurity(y[right_partition_indicies])*r_weight)\n",
    "\n",
    "        return inf_gain\n",
    "\n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        We can use this simplified version because we are solving a strictly binary classification problem, \n",
    "        assume y is a numpy array with values of 0 or 1.\n",
    "        \"\"\"\n",
    "\n",
    "        label_one_probability = sum(y)/len(y)\n",
    "\n",
    "        return 1 - ((label_one_probability)**2 + (1-label_one_probability)**2)\n",
    "    \n",
    "class TreeNode:\n",
    "    \n",
    "    def __init__(self, left, right, decision_rule):\n",
    "        \"\"\"Left and right are TreeNode objects. Decision rule is either a tuple with a feature and value \n",
    "        to split on or a single value which determines the leaf's predicted label.\n",
    "        \"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_rule = decision_rule\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.empty(len(X))\n",
    "        \n",
    "        if ((self.left is None) and (self.right is None)): # we are in a leaf node\n",
    "            return self.decision_rule\n",
    "        \n",
    "        # get left and right indices\n",
    "        left_i = np.where(X.T[self.decision_rule[0]] < self.decision_rule[1])\n",
    "        right_i = np.where(X.T[self.decision_rule[0]] >= self.decision_rule[1])\n",
    "        \n",
    "        left_prediction = self.left.predict(X[left_i])\n",
    "        right_prediction = self.right.predict(X[right_i])\n",
    "        \n",
    "        prediction[left_i] = left_prediction\n",
    "        prediction[right_i] = right_prediction\n",
    "               \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01cab634",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f300838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = T.build(X[:130] ,y[:130] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f894d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.predict(X[130:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3788b077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[130:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72638ae9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8103448275862069"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.equal(y[130:], root.predict(X[130:])))/len(np.equal(y[130:], root.predict(X[130:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd92e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
